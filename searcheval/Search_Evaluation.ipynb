{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Evaluation Notebook\n",
    "\n",
    "This python notebook will perform a search evaluation\n",
    "Prerequisites:\n",
    "* Golden data set in file ```golden_data.csv```\n",
    "* Search strategies in python modules located in the ```strategies``` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the API key to an environment variable\n",
    "import os, subprocess\n",
    "\n",
    "openai_api_key = \"YOUR KEY HERE\"\n",
    "openai_api_base = \"https://llm-proxy.prod-3.eden.elastic.dev/v1\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base\n",
    "\n",
    "subprocess.run([\n",
    "    \"deepeval\", \"set-local-model\", \n",
    "    \"--model-name=gpt-4o\", \n",
    "    f\"--base-url={openai_api_base}\", \n",
    "    f\"--api-key={openai_api_key}\"\n",
    "    f\"\"])\n",
    "\n",
    "# ## Uncomment the following lines if you want to use .env file to control settings\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What files to work with\n",
    "GOLDEN_DATA_CSV = \"golden_data.csv\"\n",
    "STRATEGIES_FOLDER = \"strategies\"\n",
    "SEARCH_OUTPUT_JSON = \"results_search_evaluation.json\"\n",
    "DEEPEVAL_OUTPUT_JSON = \"results_deepeval_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "from utility.util_es import get_es\n",
    "from utility.util_vis_result import vis_search_eval_json, vis_deep_eval_correct_tests\n",
    "from evaluate import run_search_evaluation, load_golden_data, load_strategies, output_search_eval_results\n",
    "\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html\n",
    "print(\"### Starting evaluation using Elasticsearch _rank_eval API\")\n",
    "\n",
    "# 1. Connect to Elasticsearch\n",
    "es = get_es()\n",
    "print(f\"\\tConnected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# 2. Load the golden data set\n",
    "golden_data = load_golden_data(GOLDEN_DATA_CSV)\n",
    "print(f\"\\tIdentified {len(golden_data)} golden data entry(ies) to use for search evaluation\")\n",
    "\n",
    "# 3. Load strategies from the strategies folder\n",
    "strategy_modules = load_strategies(STRATEGIES_FOLDER)  \n",
    "print(f\"\\tIdentified {len(strategy_modules)} strategy(ies) to evaluate\")\n",
    "\n",
    "# 4. Evaluate each strategy\n",
    "results = run_search_evaluation(es, golden_data, strategy_modules)\n",
    "\n",
    "# 5. Output the evaluation results\n",
    "output_search_eval_results(SEARCH_OUTPUT_JSON, results, golden_data, strategy_modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the Results of the Evaluation\n",
    "# 6. Visualize\n",
    "vis_search_eval_json(SEARCH_OUTPUT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import run_deepeval, output_deepeval_results\n",
    "\n",
    "import json\n",
    "\n",
    "# startegies to Evauate\n",
    "# include_strategy_names = [\"3b_e5_hybrid\", \"3b_e5_hybrid_qt\"]\n",
    "\n",
    "\n",
    "## Deep Eval Evaluation\n",
    "print(\"### DEEP EVAL\")\n",
    "es = get_es()\n",
    "\n",
    "rag_system_prompt = \"\"\"\n",
    "Instructions:\n",
    "\n",
    "- You are an assistant for question-answering tasks.\n",
    "- Answer questions truthfully and factually using only the context presented.\n",
    "- Do not jump to conclusions or make assumptions.\n",
    "- If the answer is not present in the provided context, just say that you don't know rather than making up an answer or using your own knowledge from outside the prompt.\n",
    "- You must always cite the document where the answer was extracted using inline academic citation style [], using the position or multiple positions. Example:  [1][3].\n",
    "- Use markdown format for code examples or bulleted lists.\n",
    "- You are correct, factual, precise, and reliable.\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "## Search Top 6 documents, with 3 inner hits per doc, send a max of 9 documents to the LLM for RAG\n",
    "deepEvalScores = run_deepeval(es, strategy_modules, golden_data, rag_system_prompt,6, 3, 9)\n",
    "\n",
    "\n",
    "## save the scores to disk\n",
    "output_deepeval_results(DEEPEVAL_OUTPUT_JSON, deepEvalScores)\n",
    "print(f\"\\nDeepEval scores saved to {DEEPEVAL_OUTPUT_JSON}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize\n",
    "vis_deep_eval_correct_tests(DEEPEVAL_OUTPUT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## We'll uncomment this in a later Exercise\n",
    "# ## Here's an alternate RAG setup sending 1/3 the tokens to the LLM to save inferences costs\n",
    "# ## We should see rescore really shine here\n",
    "\n",
    "# ## Search Top 6 documents, with 3 inner hits per doc, send a max of 3 citations to the LLM for RAG\n",
    "# deepEvalScores = run_deepeval(es, strategy_modules, golden_data, rag_system_prompt,6, 3, 3)\n",
    "\n",
    "\n",
    "# ## save the scores to disk\n",
    "# output_deepeval_results(f\"{DEEPEVAL_OUTPUT_JSON}_2\", deepEvalScores)\n",
    "# print(f\"\\nDeepEval scores saved to {DEEPEVAL_OUTPUT_JSON}_2\")\n",
    "\n",
    "# vis_deep_eval_correct_tests(f\"{DEEPEVAL_OUTPUT_JSON}_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
