{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Evaluation Notebook\n",
    "\n",
    "This python notebook will perform a search evaluation\n",
    "Prerequisites:\n",
    "* Golden data set in file ```golden_data.csv```\n",
    "* Search strategies in python modules located in the ```strategies``` folder\n",
    "* ```.env``` file with the following:\n",
    "\n",
    "```\n",
    "ES_SERVER=\"https://YOUR_ELASTICSEARCH_8_17_or_higher:443\"\n",
    "ES_API_KEY=\"ENCODED_API_KEY\"\n",
    "\n",
    "PROXY_BASE_URL=\"YOUR_KEY\"\n",
    "OPENAI_API_KEY=\"YOUR_KEY\"\n",
    "COHERE_KEY=\"YOUR_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the environment variables before importing utilities\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "from utility.util_es import get_es\n",
    "from utility.util_vis_result import vis_search_eval_json, vis_deep_eval_correct_tests\n",
    "from evaluate import run_evaluation, load_golden_data, load_strategies, output_eval_results\n",
    "\n",
    "GOLDEN_DATA_CSV = \"golden_data.csv\"\n",
    "STRATEGIES_FOLDER = \"strategies\"\n",
    "OUTPUT_CSV = \"search_evaluation_results.csv\"\n",
    "OUTPUT_JSON = \"search_evaluation_results.json\"\n",
    "\n",
    "# 1. Connect to Elasticsearch\n",
    "es = get_es()\n",
    "print(f\"\\tConnected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# 2. Load the golden data set\n",
    "golden_data = load_golden_data(GOLDEN_DATA_CSV)\n",
    "print(f\"Identified {len(golden_data)} golden data entry(ies) to use for search evaluation\")\n",
    "\n",
    "# 3. Load strategies from the strategies folder\n",
    "strategy_modules = load_strategies(STRATEGIES_FOLDER)  \n",
    "print(f\"Identified {len(strategy_modules)} strategy(ies) to evaluate\")\n",
    "\n",
    "# 4. Evaluate each strategy\n",
    "results = run_evaluation(es, golden_data, strategy_modules)\n",
    "\n",
    "# 5. Output the evaluation results\n",
    "output_eval_results(OUTPUT_CSV, OUTPUT_JSON, results, golden_data, strategy_modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the Results of the Evaluation\n",
    "\n",
    "# 6. Visualize\n",
    "vis_search_eval_json(OUTPUT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.util_deep_eval import generateLLMTestCase, evaluateTestCases\n",
    "from deepeval.evaluate import TestResult\n",
    "from utility.util_llm import LLMUtil\n",
    "from utility.util_es import search_to_context\n",
    "import os\n",
    "import json\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# startegies to Evauate\n",
    "# include_strategy_names = [\"3b_e5_hybrid\", \"3b_e5_hybrid_qt\"]\n",
    "\n",
    "\n",
    "## Deep Eval Evaluation\n",
    "print(\"### DEEP EVAL\")\n",
    "deepEvalScores = {}\n",
    "for strategy_name, module in strategy_modules.items():\n",
    "    llm_util = LLMUtil(openai_api_key)\n",
    "    # if strategy_name not in include_strategy_names:\n",
    "    #     continue\n",
    "\n",
    "    if hasattr(module, \"is_disabled\") and module.is_disabled(): ## or strategy_name != \"1a_bm25\" :\n",
    "        print(f\"Skipping strategy: {strategy_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Starting strategy: {strategy_name}\")\n",
    "    testCases = []\n",
    "    for i, item in enumerate(golden_data):\n",
    "            qid = f\"query_{i+1}\"\n",
    "            query = item[\"query\"]\n",
    "\n",
    "\n",
    "            ## correct answer from the golden data\n",
    "            correct_answer = item[\"natural_answer\"]\n",
    "\n",
    "            ## pre-process the query string\n",
    "            query_string = module.query_transform(query, llm_util,  module.get_parameters()[\"query_transform_prompt\"]) if hasattr(module, \"query_transform\") else query\n",
    "\n",
    "            ## do the RAG\n",
    "            index_name = module.get_parameters()['index_name']\n",
    "            body = module.build_query(query_string)\n",
    "            rag_context = module.get_parameters().get(\"rag_context\", \"lore\")\n",
    "            \n",
    "            retrieval_context  = search_to_context(es, index_name, body, rag_context, 3)\n",
    "            context = \"\\n\\n\".join(retrieval_context)\n",
    "            # print(f\"Context: {context}\")\n",
    "\n",
    "            system_prompt = f\"\"\"\n",
    "Instructions:\n",
    "\n",
    "- You are an assistant for question-answering tasks.\n",
    "- Answer questions truthfully and factually using only the context presented.\n",
    "- If the answer is not present in the provided context, just say that you don't know rather than making up an answer or using your own knowledge from outside the prompt.\n",
    "- You are correct, factual, precise, and reliable.\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "                \n",
    "            actual_output = llm_util.rag_cache(system_prompt, retrieval_context, query_string)\n",
    "\n",
    "\n",
    "            ## fill in query and strategy responses in score sheet\n",
    "            stratResult = {\"actual_output\": actual_output}\n",
    "            if qid not in deepEvalScores:\n",
    "                deepEvalScores[qid] = { \n",
    "                    \"query\" : query, \n",
    "                    \"correct_answer\": correct_answer,\n",
    "                    \"strategies\": { strategy_name: stratResult} }\n",
    "            else:\n",
    "                deepEvalScores[qid][\"strategies\"][strategy_name] = stratResult\n",
    "\n",
    "            ## prep deel eval test case for later batch evaluation\n",
    "            testCase = generateLLMTestCase(qid, query, actual_output, retrieval_context, correct_answer)\n",
    "            testCases.append(testCase)\n",
    "\n",
    "    ## Run evaluations for this strategy      \n",
    "    rag_evaluation = evaluateTestCases(testCases)\n",
    "\n",
    "    for test_result in  rag_evaluation.test_results:\n",
    "        quid = test_result.name\n",
    "\n",
    "        success = test_result.success\n",
    "        scores = {\"success\": success}\n",
    "        # print(f\"name: {quid} | success: {success}\")\n",
    "        for metric in  test_result.metrics_data:\n",
    "            # print(f\"{metric.name} : score {metric.score} | {metric.reason}\")\n",
    "            scores[metric.name] = {\"score\": metric.score, \"reason\": metric.reason }\n",
    "        \n",
    "        deepEvalScores[quid][\"strategies\"][strategy_name][\"scores\"] = scores\n",
    "    \n",
    "    llm_util.flush_cache()\n",
    "        \n",
    "        \n",
    "\n",
    "## save the scores to disk\n",
    "# print(json.dumps(deepEvalScores, indent=2))\n",
    "with open(\"deepeval_results.json\", \"w\") as f:\n",
    "    json.dump(deepEvalScores, f, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize\n",
    "vis_deep_eval_correct_tests(\"deepeval_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
