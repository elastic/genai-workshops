{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Evaluation Notebook\n",
    "\n",
    "This python notebook will perform a search evaluation \n",
    "Prerequisites:\n",
    "* Golden data set in file ```golden_data.csv```\n",
    "* Search strategies in python modules located in the ```strategies``` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os, subprocess\n",
    "\n",
    "# Export the API key to an environment variable\n",
    "if not os.path.exists('.env.instruqt'):\n",
    "    env_text = requests.get('http://kubernetes-vm:9000/env').text\n",
    "    with open('.env.instruqt', 'w') as f:\n",
    "        f.write(env_text)\n",
    "load_dotenv('.env.instruqt')\n",
    "\n",
    "\n",
    "# # # ## Uncomment the following lines if you want to use .env file to control settings\n",
    "# load_dotenv()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What files to work with\n",
    "GOLDEN_DATA_CSV = \"judgement_list.csv\"\n",
    "STRATEGIES_FOLDER = \"strategies\"\n",
    "SEARCH_OUTPUT_JSON = \"results_search_evaluation.json\"\n",
    "DEEPEVAL_OUTPUT_JSON = \"results_deepeval_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "from utility.util_es import get_es\n",
    "from utility.util_vis_result import vis_search_eval_json, vis_deep_eval_correct_tests\n",
    "from evaluate import run_search_evaluation, load_golden_data, load_strategies, output_search_eval_results\n",
    "\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html\n",
    "print(\"### Starting evaluation using Elasticsearch _rank_eval API\")\n",
    "\n",
    "# 1. Connect to Elasticsearch\n",
    "es = get_es()\n",
    "print(f\"\\tConnected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# 2. Load the golden data set\n",
    "golden_data = load_golden_data(GOLDEN_DATA_CSV)\n",
    "print(f\"\\tIdentified {len(golden_data)} golden data entry(ies) to use for search evaluation\")\n",
    "\n",
    "# 3. Load strategies from the strategies folder\n",
    "strategy_modules = load_strategies(STRATEGIES_FOLDER)  \n",
    "print(f\"\\tIdentified {len(strategy_modules)} strategy(ies) to evaluate\")\n",
    "\n",
    "# 4. Evaluate each strategy\n",
    "results = run_search_evaluation(es, golden_data, strategy_modules)\n",
    "\n",
    "# 5. Output the evaluation results\n",
    "output_search_eval_results(SEARCH_OUTPUT_JSON, results, golden_data, strategy_modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the Results of the Evaluation\n",
    "# 6. Visualize\n",
    "vis_search_eval_json(SEARCH_OUTPUT_JSON)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
