{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Evaluation Notebook\n",
    "\n",
    "This python notebook will perform a search evaluation \n",
    "Prerequisites:\n",
    "* Golden data set in file ```golden_data.csv```\n",
    "* Search strategies in python modules located in the ```strategies``` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os, subprocess\n",
    "\n",
    "# Export the API key to an environment variable\n",
    "if not os.path.exists('.env.instruqt'):\n",
    "    env_text = requests.get('http://kubernetes-vm:9000/env').text\n",
    "    with open('.env.instruqt', 'w') as f:\n",
    "        f.write(env_text)\n",
    "load_dotenv('.env.instruqt')\n",
    "\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base\n",
    "\n",
    "subprocess.run([\n",
    "    \"deepeval\", \"set-local-model\", \n",
    "    \"--model-name=eval-gpt-4o\", ## needs azure 2024-11-20 +\n",
    "    f\"--base-url={openai_api_base}\", \n",
    "    f\"--api-key={openai_api_key}\"\n",
    "    f\"\"])\n",
    "\n",
    "# # ## Uncomment the following lines if you want to use .env file to control settings\n",
    "# load_dotenv()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What files to work with\n",
    "GOLDEN_DATA_CSV = \"wikivoyage_judgement_list.csv\"\n",
    "STRATEGIES_FOLDER = \"strategies\"\n",
    "SEARCH_OUTPUT_JSON = \"results_search_evaluation.json\"\n",
    "DEEPEVAL_OUTPUT_JSON = \"results_deepeval_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "from utility.util_es import get_es\n",
    "from utility.util_vis_result import vis_search_eval_json, vis_deep_eval_correct_tests\n",
    "from evaluate import run_search_evaluation, load_golden_data, load_strategies, output_search_eval_results\n",
    "\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html\n",
    "print(\"### Starting evaluation using Elasticsearch _rank_eval API\")\n",
    "\n",
    "# 1. Connect to Elasticsearch\n",
    "es = get_es()\n",
    "print(f\"\\tConnected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# 2. Load the golden data set\n",
    "golden_data = load_golden_data(GOLDEN_DATA_CSV)\n",
    "print(f\"\\tIdentified {len(golden_data)} golden data entry(ies) to use for search evaluation\")\n",
    "\n",
    "# 3. Load strategies from the strategies folder\n",
    "strategy_modules = load_strategies(STRATEGIES_FOLDER)  \n",
    "print(f\"\\tIdentified {len(strategy_modules)} strategy(ies) to evaluate\")\n",
    "\n",
    "# 4. Evaluate each strategy\n",
    "results = run_search_evaluation(es, golden_data, strategy_modules)\n",
    "\n",
    "# 5. Output the evaluation results\n",
    "output_search_eval_results(SEARCH_OUTPUT_JSON, results, golden_data, strategy_modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the Results of the Evaluation\n",
    "# 6. Visualize\n",
    "vis_search_eval_json(SEARCH_OUTPUT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import run_deepeval, output_deepeval_results\n",
    "\n",
    "import json\n",
    "\n",
    "# startegies to Evauate\n",
    "# include_strategy_names = [\"3b_e5_hybrid\", \"3b_e5_hybrid_qt\"]\n",
    "\n",
    "\n",
    "## Deep Eval Evaluation\n",
    "print(\"### DEEP EVAL\")\n",
    "es = get_es()\n",
    "\n",
    "rag_system_prompt = \"\"\"\n",
    "Instructions:\n",
    "\n",
    "- You are an assistant for question-answering tasks.\n",
    "- Answer questions using only the context presented.\n",
    "- If the answer is not present in the provided context, just say that you don't know rather than making up an answer or using your own knowledge from outside the prompt.\n",
    "- You must always cite the document where the answer was extracted using inline academic citation style [], using the position or multiple positions. Example:  [1][3].\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "citations_max = 3  ### <--- reducing the number of sub-document citiations shows the value of ranking to RAG (3 hard, 9 easy)\n",
    "\n",
    "## Search Top 6 documents, with 3 inner hits per doc, send a max of X citations to the LLM for RAG\n",
    "deepEvalScores = run_deepeval(es, strategy_modules, golden_data, rag_system_prompt,6, 3, citations_max)\n",
    "\n",
    "\n",
    "## save the scores to disk\n",
    "output_deepeval_results(DEEPEVAL_OUTPUT_JSON, deepEvalScores)\n",
    "print(f\"\\nDeepEval scores saved to {DEEPEVAL_OUTPUT_JSON}\")\n",
    "# Visualize\n",
    "vis_deep_eval_correct_tests(DEEPEVAL_OUTPUT_JSON, f\"DeepEval Correctness ({citations_max} citations)\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
