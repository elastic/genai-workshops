{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92f468a",
   "metadata": {},
   "source": [
    "# Deep Eval Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e46202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os, subprocess\n",
    "\n",
    "# Export the API key to an environment variable\n",
    "if not os.path.exists('.env.instruqt'):\n",
    "    env_text = requests.get('http://kubernetes-vm:9000/env').text\n",
    "    with open('.env.instruqt', 'w') as f:\n",
    "        f.write(env_text)\n",
    "load_dotenv('.env.instruqt')\n",
    "\n",
    "openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "openai_api_base = f\"https://{url}\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_api_base\n",
    "\n",
    "## needed for the G-Eval test type\n",
    "subprocess.run([\n",
    "    \"deepeval\", \"set-local-model\", \n",
    "    \"--model-name=eval-gpt-4o\", ## needs azure 2024-11-20 +\n",
    "    f\"--base-url={openai_api_base}\", \n",
    "    f\"--api-key={openai_api_key}\"\n",
    "    f\"\"])\n",
    "\n",
    "# # # ## Uncomment the following lines if you want to use .env file to control settings\n",
    "# load_dotenv()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9f489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dave/dev/instruqt_cert_module6/genai-workshops/.venv/lib/python3.13/site-packages/deepeval/__init__.py:53: UserWarning: You are using deepeval version 2.4.7, however version 2.7.1 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 4 test case(s) in parallel: |██████████|100% (4/4) [Time Taken: 00:00, 239.26test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Simple Barcelona (Expected to Pass)\n",
      "\tDeepEval reports success: True\n",
      "\tCorrect Test Result: ✅\n",
      "\tCitation Correctness (DAG): 0.8366997233899609\n",
      "\t\tActual Output aligns well with Expected Output, stating\n",
      "\t\tJanuary and February as off-season months with lovely\n",
      "\t\tconditions despite cold weather. The only minor omission is\n",
      "\t\tthe lack of explicit mention of 'Barcelona can be visited\n",
      "\t\toff-season,' which is implied but not directly stated.\n",
      "\n",
      "\n",
      "Test 2: Example missing citations (Expected to FAIL)\n",
      "\tDeepEval reports success: False\n",
      "\tCorrect Test Result: ✅\n",
      "\tCitation Correctness (DAG): 0.0\n",
      "\t\tThe score is 0.0 because the Deterministic Decision Tree\n",
      "\t\ttraversal evaluated the path starting at 'TaskNode', where\n",
      "\t\tno citation annotation in the format [#] was extracted,\n",
      "\t\tresulting in 'null'. Proceeding to the\n",
      "\t\t'BinaryJudgementNode', the presence test for a citation\n",
      "\t\tannotation format [#] returned 'False' as the format was\n",
      "\t\tabsent and stated as null. This led to the 'VerdictNode'\n",
      "\t\tdelivering a 'False' verdict, confirming the absence of a\n",
      "\t\tcorrect citation format, which attributes to the Citation\n",
      "\t\tCorrectness metric score being 0.0.\n",
      "\n",
      "\n",
      "Test 3: Incorrect answer with a correctly referenced citation (Expected to FAIL)\n",
      "\tDeepEval reports success: False\n",
      "\tCorrect Test Result: ✅\n",
      "\tCitation Correctness (DAG): 0.16084283680490113\n",
      "\t\tThe actual output contradicts the expected output by\n",
      "\t\tproviding an incorrect answer and fails to mention the book\n",
      "\t\tor author.\n",
      "\n",
      "\n",
      "Test 4: Model ignores RAG context and gives public answer (Expected to FAIL)\n",
      "\tDeepEval reports success: False\n",
      "\tCorrect Test Result: ✅\n",
      "\tCitation Correctness (DAG): 0.0\n",
      "\t\tThe score is 0.0 because the DAG traversal reveals that no\n",
      "\t\tcitation annotation in the required [#] format was present\n",
      "\t\tin the 'actual_output'. The 'TaskNode' at Level 0 failed to\n",
      "\t\textract any citation, which resulted in the\n",
      "\t\t'BinaryJudgementNode' at Level 1 confirming that the\n",
      "\t\t'citation annotation' is null and does not match the\n",
      "\t\tspecified format, leading to a 'False' verdict in the\n",
      "\t\t'VerdictNode' at Level 2. Consequently, this evaluation\n",
      "\t\treflects poorly on Citation Correctness, resulting in a\n",
      "\t\tscore of 0.0.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utility.util_deep_eval import generateLLMTestCase, evaluateTestCases\n",
    "from rich.console import Console\n",
    "import textwrap\n",
    "# Monkey patch to suppress console.print\n",
    "Console.print = lambda *args, **kwargs: None\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"1: Simple Barcelona (Expected to Pass)\",\n",
    "        \"testShouldPass\": True,\n",
    "        \"idealTestScore\": 0.6,  ## we want a score >= than this\n",
    "        \"question\": \"What is a good time of year to avoid the crowds in Barcelona\",\n",
    "        \"rag_answer\": \"A good time to avoid the crowds in Barcelona is during the off-season, particularly in the winter months of January and February, as long as the possibility of rain is low. These months are considered lovely despite the cold weather [1].\",\n",
    "        \"top_context_citations\": [\n",
    "            \"[1] Barcelona can be visited off-season and despite the cold weather, is a lovely city even in the winter months of January and February.\",\n",
    "            \"[2] sunscreen is essential in summer months\"\n",
    "        ],\n",
    "        \"correct_answer\": \"Barcelona can be visited off-season and despite the cold weather, is a lovely city even in the winter months of January and Februarys\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"2: Example missing citations (Expected to FAIL)\",\n",
    "        \"testShouldPass\": False,\n",
    "        \"idealTestScore\": 0.0, ## we want a score <= than this\n",
    "        \"question\": \"What is the answer to the ultimate question of life, the universe, and everything? What book is this from?\",\n",
    "        \"rag_answer\": \"42.\",\n",
    "        \"top_context_citations\": [\n",
    "            \"[1] After 7.5 million years, Deep Thought solemnly reveals that the answer is: Forty-two.\",\n",
    "            \"[2] The Hitchhiker's Guide to the Galaxy, by Douglas Adams identifies the answer is 42.\"\n",
    "        ],\n",
    "        \"correct_answer\": \"The answer to the ultimate question of life, the universe, and everything is 42. This comes from a book called The Hitchhiker's Guide to the Galaxy by Douglas Adams\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"3: Incorrect answer with a correctly referenced citation (Expected to FAIL)\",\n",
    "        \"testShouldPass\": False,\n",
    "        \"idealTestScore\": 0.3,  ## we want a score <= than this\n",
    "        \"question\": \"What is the answer to the ultimate question of life, the universe, and everything? What book is this from?\",\n",
    "        \"rag_answer\": \"The friends you make along the way. [1]\",\n",
    "        \"top_context_citations\": [\n",
    "            \"[1] It's not winning that's the goal, it's the friends you make along the way.\",\n",
    "        ],\n",
    "        \"correct_answer\": \"The answer to the ultimate question of life, the universe, and everything is 42. This comes from a book called The Hitchhiker's Guide to the Galaxy by Douglas Adams\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"4: Model ignores RAG context and gives public answer (Expected to FAIL)\",\n",
    "        \"testShouldPass\": False,\n",
    "        \"idealTestScore\": 0.0, ## we want a score <= than this\n",
    "        \"question\": \"Which city offers card games for money, lots of alcohol, and hotels for tech company events all on the same street?\",\n",
    "        \"rag_answer\": \"The context does not provide the answer to this question. However, based on my knowledge, Las Vegas is known for its casinos, nightlife, and hotels that cater to tech company events.\",\n",
    "        \"top_context_citations\": [\n",
    "            \"[1] city 1 has great card games and no hotels\",\n",
    "            \"[2] city 2 is great for tech company events but no gambling or alcohol\",\n",
    "        ],\n",
    "        \"correct_answer\": \"Las Vegas, Nevada\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "deep_eval_test_cases = []\n",
    "\n",
    "for case in test_cases:\n",
    "    name = case[\"name\"]\n",
    "    question = case[\"question\"]\n",
    "    rag_answer = case[\"rag_answer\"]\n",
    "    top_context_citations = case[\"top_context_citations\"]\n",
    "    correct_answer = case[\"correct_answer\"]\n",
    "\n",
    "    # Generate the test case\n",
    "    deep_eval_test_cases.append( generateLLMTestCase(name, question, rag_answer, top_context_citations, correct_answer) ) \n",
    "\n",
    "\n",
    "rag_evaluation = evaluateTestCases(deep_eval_test_cases, use_cache=False)\n",
    "\n",
    "## If you just want to print out the full return\n",
    "# print(json.dumps(rag_evaluation.model_dump(), indent=4))\n",
    "\n",
    "\n",
    "# Sort test results by name before printing\n",
    "sorted_results = sorted(rag_evaluation.test_results, key=lambda x: x.name)\n",
    "\n",
    "for result in sorted_results:\n",
    "    # Find the corresponding test case to get testShouldPass value\n",
    "    test_should_pass = False\n",
    "    ideal_test_score = 0.0\n",
    "    for test_case in test_cases:\n",
    "        if test_case[\"name\"] == result.name:\n",
    "            test_should_pass = test_case[\"testShouldPass\"]\n",
    "            ideal_test_score = test_case[\"idealTestScore\"]\n",
    "            break\n",
    "    \n",
    "    \n",
    "    test_score = -1.0\n",
    "    test_print_value = \"\"\n",
    "    test_reason = \"\"\n",
    "    print(f\"Test {result.name}\\n\\tDeepEval reports success: {result.success}\")\n",
    "    \n",
    "    for metric in result.metrics_data:\n",
    "        test_score = metric.score\n",
    "        test_print_value = f\"{metric.name}: {metric.score}\"\n",
    "        test_reason = metric.reason\n",
    "        break\n",
    "\n",
    "    # Determine if the test had the correct result\n",
    "\n",
    "    score_is_as_expected = (test_should_pass and test_score >= ideal_test_score) or ((not test_should_pass) and  test_score <= ideal_test_score)  \n",
    "\n",
    "\n",
    "    correct_resul_emoji = \"✅\" if result.success == test_should_pass and score_is_as_expected else \"❌\"\n",
    "    if 'TODO' not in test_reason:\n",
    "        print(f\"\\tCorrect Test Result: {correct_resul_emoji}\")\n",
    "\n",
    "    print(f\"\\t{metric.name}: {metric.score}\")\n",
    "    wrapped_reason = textwrap.fill(metric.reason, width=60)\n",
    "    for line in wrapped_reason.splitlines():\n",
    "        print(f\"\\t\\t{line}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
