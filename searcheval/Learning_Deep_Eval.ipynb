{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92f468a",
   "metadata": {},
   "source": [
    "# What? \n",
    "Let's go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e46202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "import os, subprocess\n",
    "\n",
    "# # Export the API key to an environment variable\n",
    "# if not os.path.exists('.env.instruqt'):\n",
    "#     env_text = requests.get('http://kubernetes-vm:9000/env').text\n",
    "#     with open('.env.instruqt', 'w') as f:\n",
    "#         f.write(env_text)\n",
    "# load_dotenv('.env.instruqt')\n",
    "\n",
    "# openai_api_key =  os.environ.get(\"LLM_APIKEY\") \n",
    "# url = os.environ.get(\"LLM_PROXY_URL\") \n",
    "# openai_api_base = f\"https://{url}\"\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = openai_api_base\n",
    "\n",
    "# subprocess.run([\n",
    "#     \"deepeval\", \"set-local-model\", \n",
    "#     \"--model-name=eval-gpt-4o\", ## needs azure 2024-11-20 +\n",
    "#     f\"--base-url={openai_api_base}\", \n",
    "#     f\"--api-key={openai_api_key}\"\n",
    "#     f\"\"])\n",
    "\n",
    "# # ## Uncomment the following lines if you want to use .env file to control settings\n",
    "load_dotenv()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9f489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:00, 176.12test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"test_results\": [\n",
      "        {\n",
      "            \"name\": \"1\",\n",
      "            \"success\": true,\n",
      "            \"metrics_data\": [\n",
      "                {\n",
      "                    \"name\": \"Citation Correctness (DAG)\",\n",
      "                    \"threshold\": 0.5,\n",
      "                    \"success\": true,\n",
      "                    \"score\": 0.6008235257966291,\n",
      "                    \"reason\": \"The actual output correctly states that the answer is 42, aligning with the expected output. However, it omits the detail about the source being 'The Hitchhiker's Guide to the Galaxy by Douglas Adams.'\",\n",
      "                    \"strict_mode\": false,\n",
      "                    \"evaluation_model\": \"gpt-4o\",\n",
      "                    \"error\": null,\n",
      "                    \"evaluation_cost\": 0.0,\n",
      "                    \"verbose_logs\": \"______________________\\n| TaskNode | Level == 0 |\\n*******************************\\nLabel: None\\n\\nInstructions:\\nExtract the citation annotation of format [#] used in the answer `actual_output`, if no ciation is present return null\\n\\ncitation annotation:\\n[1][2]\\n \\n \\n__________________________________\\n| BinaryJudgementNode | Level == 1 |\\n************************************************\\nLabel: None\\n\\nCriteria:\\ntest whether `citation annotation` is present in the format [#] and not null\\n\\nVerdict: True\\nReason: The presence of citation annotations [1][2] fits the required format [#] and both annotations are not null or empty. Thus, the condition is satisfied.\\n \\n \\n________________________\\n| VerdictNode | Level == 2 |\\n**********************************\\nVerdict: True\\nType: GEval\\n\\nCriteria:\\nNone\\nEvaluation Steps:\\n[\\n    \\\"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\\\",\\n    \\\"You should also moderately penalize omission of detail\\\",\\n    \\\"Vague language, or contradicting OPINIONS, are not OK\\\",\\n    \\\"do not check, comment, or penalize whether 'expected output' has citations\\\"\\n]\"\n",
      "                }\n",
      "            ],\n",
      "            \"conversational\": false,\n",
      "            \"multimodal\": false,\n",
      "            \"input\": \"What is the answer to the ultimate question of life, the universe, and everything? What book is this from?\",\n",
      "            \"actual_output\": \"The answer is 42 [1][2]\",\n",
      "            \"expected_output\": \"The answer to the ultimate question of life, the universe, and everything is 42. This comes from a book called The Hitchhiker's Guide to the Galaxy by Douglas Adams\",\n",
      "            \"context\": null,\n",
      "            \"retrieval_context\": [\n",
      "                \"[1] After 7.5 million years, Deep Thought solemnly reveals that the answer is: Forty-two.\",\n",
      "                \"[2] The Hitchhiker's Guide to the Galaxy, by Douglas Adams identifies the answer is 42.\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"confident_link\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utility.util_deep_eval import generateLLMTestCase, evaluateTestCases\n",
    "from rich.console import Console\n",
    "# Monkey patch to suppress console.print\n",
    "Console.print = lambda *args, **kwargs: None\n",
    "\n",
    "question = \"What is the answer to the ultimate question of life, the universe, and everything? What book is this from?\"\n",
    "rag_answer = \"The answer is 42 [1][2]\"\n",
    "top_context_citations = [\n",
    "    \"[1] After 7.5 million years, Deep Thought solemnly reveals that the answer is: Forty-two.\",\n",
    "    \"[2] The Hitchhiker's Guide to the Galaxy, by Douglas Adams identifies the answer is 42.\"\n",
    "]\n",
    "correct_answer = \"The answer to the ultimate question of life, the universe, and everything is 42. This comes from a book called The Hitchhiker's Guide to the Galaxy by Douglas Adams\"\n",
    "\n",
    "\n",
    "testCase = generateLLMTestCase(\"1\", question, rag_answer, top_context_citations, correct_answer)\n",
    "rag_evaluation = evaluateTestCases([testCase])\n",
    "\n",
    "print(json.dumps(rag_evaluation.model_dump(), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
